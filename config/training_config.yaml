# Training Configuration
training:
  # General settings
  output_dir: "./results/checkpoints"
  seed: 42
  fp16: true
  bf16: false
  
  # Training hyperparameters
  num_train_epochs: 3
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 4
  gradient_checkpointing: true
  
  # Optimizer settings
  learning_rate: 2e-4
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8
  max_grad_norm: 1.0
  
  # Learning rate schedule
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.03
  warmup_steps: 0
  
  # Logging & Evaluation
  logging_dir: "./results/logs"
  logging_steps: 10
  eval_strategy: "steps"
  eval_steps: 100
  save_strategy: "steps"
  save_steps: 500
  save_total_limit: 3
  load_best_model_at_end: true
  metric_for_best_model: "execution_accuracy"
  greater_is_better: true
  
  # Early stopping
  early_stopping_patience: 3
  early_stopping_threshold: 0.01
  
  # Reporting
  report_to: ["tensorboard", "wandb"]
  run_name: null  # Will be set dynamically

# Data Configuration
data:
  train_size: 0.8
  val_size: 0.1
  test_size: 0.1
  max_samples: null  # Use all data, or set a limit for quick testing
  shuffle: true
  
  # Data augmentation
  augmentation:
    paraphrase: true
    synonym_replacement: false
    back_translation: false

# Evaluation Configuration
evaluation:
  batch_size: 16
  metrics:
    - exact_match
    - execution_accuracy
    - component_match
    - valid_sql
  
  timeout: 10  # SQL execution timeout in seconds
  max_retries: 3
