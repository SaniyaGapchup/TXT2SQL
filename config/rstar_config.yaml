# rStar-SQL Training Configuration
# Configuration for deep thinking Text-to-SQL with MCTS and self-evolution

# Evolution Settings
evolution:
  num_rounds: 5  # Number of self-evolution rounds
  cot_examples_per_round: 1000  # CoT examples to generate per round
  save_every_round: true
  eval_every_round: true

# MCTS Settings
mcts:
  num_simulations: 100  # Number of simulations per question
  c_puct: 1.4  # Exploration constant
  temperature: 1.0  # Sampling temperature
  min_reward_threshold: 0.7  # Minimum reward for CoT examples
  diversity_weight: 0.3  # Weight for diversity in example selection

# Policy Model Training
policy_training:
  epochs: 3  # Epochs per round
  batch_size: 16
  learning_rate: 2.0e-5
  warmup_steps: 100
  max_length: 512
  gradient_accumulation_steps: 4
  weight_decay: 0.01
  max_grad_norm: 1.0

# Process Preference Model Training
ppm_training:
  epochs: 2  # Epochs per round
  batch_size: 16
  learning_rate: 1.0e-5
  warmup_steps: 50
  num_trajectory_pairs: 500  # Pairs to collect per round

# Model Settings
model:
  base_model: "microsoft/phi-2"  # Base SLM to use
  load_in_8bit: false
  load_in_4bit: false
  use_flash_attention: false

# Data Settings
data:
  dataset: "spider"  # spider or wikisql
  data_dir: "data"
  max_train_samples: null  # null for all
  max_eval_samples: 100  # For faster evaluation

# Logging
logging:
  use_wandb: false  # Enable Weights & Biases
  wandb_project: "rstar-sql"
  wandb_entity: null
  log_every_n_steps: 10
  save_steps: 500

# Evaluation
evaluation:
  metrics:
    - execution_accuracy
    - exact_match
    - valid_sql_rate
    - component_match
  eval_batch_size: 8
  num_eval_samples: 100

# Output
output:
  output_dir: "checkpoints/rstar"
  save_total_limit: 3  # Keep only last 3 checkpoints
  save_cot_data: true
  save_trajectory_pairs: true

# Hardware
hardware:
  device: "cuda"  # cuda or cpu
  fp16: true
  bf16: false
  gradient_checkpointing: true
